\documentclass[./main.tex]{subfiles}

\begin{document}
\section{Discussion}
\label{sec:discussion}
The following section discusses our obtained results from the pretraining and finetuning stages, some concerns regarding possible suboptimal choices we have made, as well as suggests some possible future work. The section begins in section \ref{sec:results}, where we discuss the obtain results. Then, in section \ref{sec:pretrain_vs_finetune}, where we discuss why the models performed much better during the finetuning stage, than during the pretraining stage. Section \ref{sec:best_model} then discusses which model is the optimal choice. In section \ref{sec:general_reflections} we then discuss some pitfalls of our approach and how we could have made some better choices. Lastly, in section \ref{sec:future_work} we present some possible future work possibilities.

\subsection{Results}
\label{sec:results}
In section \ref{sec:pretraining} and section \ref{sec:finetuning} we successfully implemented and tested the four architectures in various experiments. We will in the following be analysing and discussing the potential reasons for these obtained results.

\subsubsection{Pretraining}
We saw in section \ref{subsec:pretrain_test_res} how 3DConv tend to perform much better in experiment 2, than it does in experiment 1 (with the exception of run 2.1 and 2.2). As the goal of experiment 1 is for the models to both learn translation and scaling, whereas the goal of experiment 2 is just for the models to learn translation, we can clearly see here how big of a difference the task of learning to also scale the data has on the results for 3DConv. Of course, a tiny bit of randomness does also play in here, as the models for the two experiments were initialized with different weights. However, as the weights were sampled from the same distribution, as well as 3DConv having very few weights, we find it hard to believe that this randomness has such a big impact on the results. Thus we claim that the performance differences between the two experiments is due to the task of scaling the data. For the two architectures that are based on a bidirectional convolutional LSTM, a similar pattern is observed for the shifting scalar, however, this performance difference is much smaller than it was in the case for 3DConv. We simply believe, that this improved performance is due to the bidirectional convolutional LSTMs predicting the position of the keypoints in a more sophisticated manner. For DeciWatch there are no performance differences between experiment 1 and experiment 2 when considering shifting-scalar. However, this is expected, as unlike the other architectures, DeciWatch works directly on keypoint-coordinates instead of the heatmaps, making it scaling invariant and thus making it the two experiments equivalent to each other. 
\\
\\
We further saw in section \ref{subsec:pretrain_test_res} the effects on the models of halfing the frame rate. For both the 3-dimensional convolutional LSTM and DeciWatch this had a negative effect, whereas it actually had a positive effect for the two models that are based on a bidirectional LSTM. For DeciWatch we have to keep in mind, that it is already only considering every fifth frame, so by halfing the frame rate it is actually only considering every tenth frame, removing a lot of the context. For 3DConv and the two models based on a bidirectional convolutional LSTM, we again suggest that the performance difference is due to the later models yielding their predictions in a more sophisticated manner than the simple 3DConv. In the case of when the shifting-scalar is $s = 2$ all models generally perform worse in experiment 3 than they do in experiment 1, which we simply believe is due to the data being so noisy, that the models need the finer details to infer their predictions. As the distance-threshold for PCK is increased however, 3DConv actually perform better in experiment 3 than it does in experiment 1, contradicting this belief. However, we have to note here, that the performance difference is rather small and that it is in the case where the the distance-threshold is at its greatest. Thus, we believe the performance difference is due to the random initialization of the models or the models learning a rough estimate of the keypoints over learning a very fine estimation as this is very difficult when the data is very noisy. We further strengthen our last believe when considering, that the model is the lowest performing model when considering PCK@0.05 but the highest performing model when considering PCK@0.2 and thus generally learns to yield a rough estiamte of the position of the keypoints.
\\
\\
We finally saw in \ref{subsec:pretrain_test_res} how the models for shifting-scalar $s = 1$ had the most difficulty with the wrists and ankles, whereas they performed the best on the ears and shoulders. However, we find this very expected, as the wrists and ankles tend to be the joints that have the highest amount of movements, especially in sports which our pretraining dataset is centered around, and thus are much more difficult to denoise than more static joints such as the ears and shoulders. On the other hand, in the case of using the shifting-scalar $s = 2$ we saw, that the models struggled the most with the elbows and knees, and performed the best on the ears, nose and the ankles. At first glance, one would find this rather unexpecting as the wrists and ankles are no longer the most difficult keypoints. However, we suggest that this could be caused by these keypoints being too close to the heatmap borders, such that when they are shifted a lot towards any of these borders, they will end up at the border, as they cannot exceed the border. Thus, the wrists and ankles are actually not shifted as far as the other keypoints, such as the knees and elbows which would then be the most difficult keypoints, as these keypoints are both shifted a lot and the corresponding joints contain a decent amount of movement.

\subsubsection{Finetuning}
In section \ref{subsec:finetune_train_val_res} we saw how 3DConv generally converged towards the highest validation accuracy, whereas the DeciWatch-based models and the models based on a bidirectional convolutional LSTM tended to overfit and thus yielding worse validation results as the training continues.The ClimbAlong-dataset is very small and these models are much bigger than 3DConv (in terms of tuneable parameters), which means that they have a greater likelihood of "remembering" the ClimbAlong-dataset instead of learning its patterns and thus overfitting. We do however find it odd, that the DeciWatch models from experiment 3 are not overfitting. We are not too sure why these two models are not overfitting.
\\
\\
In section \ref{subsec:finetune_test_res} we saw how the models from the experiments with shifting-scalar $s = 1$ tend to yield better results than the models trained on data with shifting-scalar $s = 2$. We further saw, how the models from experiment 2 generally performs better than the models from experiment 1. Generally however, these performance differences are very minor, which makes sense as the finetuning-data is the same for the two groups. We do note the minor performance differences however, which probably means, that (1) the noise in the finetuning data is more similar to the data with shifting-scalar $s = 1$ than the data with shifting-scalar $s = 2$, and (2) the standard deviation of the peaks of the finetuning data is not changing as much as they do in our experiment 2. 
\\
\\
One could look at Figure \ref{fig:finetune_res} and claim that the pretraining-stage has had no effect, as the validation accuracy starts off very low. However, we disagree for two reasons. First off, we just noted that the choice of shifting-scalar from the pretraining-stage had an effect on the final results, so the pretraining-stage must have had an effect. Secondly, most of the models actually reach a very high validation accuracy after just a couple of epochs, which we believe is due to the pretraining. One could argue, that a similar pattern was observed during the pretraining-stage and is thus just a general pattern. However, we have to remember, that the pretraining-dataset is much bigger than the finetuning-dataset and thus just after one epoch in the pretraining-stage, the models have already been trained on a lot of samples, which explains this major performance-jump during the pretraining-stage.
\\
\\
Lastly, one saw in section \ref{subsec:finetune_test_res} how the models tend to perform the worst on keypoints related to the thumbs, pinkies and index fingers. We see two reasons for this observation. First off, these keypoints were not included in the pretraining dataset, meaning the models have learned to infer the position of these keypoints purely from the finetuning dataset. We believe this reason to only have a minor effect, as if we look at the performance of the models on the keypoints related to the heels and the toes, which were also not included in the pretraining dataset, we see, that the models generally performs only a tiny bit worse on these keypoints, that on the remaining keypoints. The second reason is, that the finger keypoints just have a lot of movement, as the related joints are the joints that are mostly used for bouldering and ara thus the joints that moves the most. And as previously stated, keypoints that move a lot are more difficult to predict the position of.

\subsection{Why did the Models Perform Better during Finetuning than during Pretraining?}
\label{sec:pretrain_vs_finetune}
All of the finetuned models outperforms their pretrained counterpart. We see a couple of reasons for this. 
\\
\\
First off, the models have been trained om more data, as they have been trained on both the pretraining data and the finetuning data. Secondly, the various videos of the ClimbAlong dataset are very semantically similar. One the other hand, the pretraining dataset consisted of people breakdancing, throwing a baseball, bench pressing and performing sit ups, which are all very semantically different from each other, which could make it hard for the models to learn to generalize. Similarly, the samples from the pretraining dataset were all shot from very similar camera ankles, whereas the samples from the pretraining data were filmed from all kinds of camera ankles, resulting in the same keypoints being placed at different locations
\\
\\
Further, the BRACE dataset was not fully manually annotated, but instead annotated using a pose estimator and only certain incorrectly predicted poses were manually annotated, where incorrectly predicted poses were detected by a machine learning model. However, if either the machine learning model for detecting incorrectly predicted poses or the pose estimator delivered suboptimal results, then the annotations would also be suboptimal and thus contain some noise, which is very difficult for our models to replicate. The finetuning dataset on the other hand was completely fully annotated by humans, making the annotations much more consistent and thus easier for the models to replicate.
\\
\\
We also see a problem with the Penn Action dataset, which can have an effect on the pretraining results. For the BRACE dataset, it is clearly documented, that the video sequences is filmed using 30 frames per second. For Penn Action, on the other hand, a similar information is not stated anywhere and the video sequences come as individual frames without the duration of the video sequences noted, hence why we cannot either compute the frame rate of these video sequences. This could cause some problems, as two sample windows, one from each dataset, could span two different durations and thus capture two different duration of context, which can confuse the developed models. On the other hand, all of the video sequences of the finetuning dataset are all filmed using 30 frames per second, making the dataset much more consistent, which again makes the fitting easier.
\\
\\
Lastly, one major reason behind the performance differences behind the models on the pretraining dataset and the finetuning dataset is the perforamnce of the identity functions. If we compare the performance of the identity function on the pretraining dataset in Table \ref{tab:pretrain_test_accs_1} and Table \ref{tab:pretrain_test_accs_2} against the performance of the identity function on the finetuning dataset in Table \ref{tab:finetune_test_accs_1} and Table \ref{tab:finetune_test_accs_2}, we clearly see, that the identity function performs much better during the finetuning than during the pretraining. Thus, a lot less temporal smoothing has to be done by our temporal-inclusive models, making the learning a lot easier.

\subsection{Which Model is the Best for Temporal Smoothing?}
\label{sec:best_model}
\begin{table}[htbp]
    \centering
    \begin{tabular}{c||cccccc|c}
        \hline
        & 1.1 & 1.2 & 1.3 & 2.1 & 2.2 & 2.3 & Total \\
        \hline
        \hline
        3DConv & 81.5 & 82.4 & 82.7 & 80.4 & 82.1 & \textbf{80.8} & 81.7 \\
        DeciWatch & \textbf{90.1} & \textbf{90.1} & \textbf{83.8} & \textbf{89.8} & \textbf{89.8} & 68.2 & \textbf{85.3} \\
        bi-Conv LSTM Model S & 76.3 & 75.6 & 76.8 & 77.0 & 76.0 & 75.5 & 76.2 \\
        bi-Conv LSTM Model C & 76.3 & 77.4 & 77.3 & 77.1 & 77.2 & 76.4 & 77.0 \\
        \hline
    \end{tabular}
    \label{tab:finetune_mean_res}
    \caption{Mean of testing PCK$@0.05$, PCK$@0.1$, and PCK$@0.2$ of each finetuning run.}
\end{table}\

\begin{table}[htbp]
    \centering
    \begin{tabular}{c||ccc}
        \hline
         & \begin{tabular}[c]{@{}c@{}}Mean\\prediction time (ms) \end{tabular} & \begin{tabular}[c]{@{}c@{}}Standard deviation\\of prediction time (ms)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Number of\\parameters\end{tabular} \\
        \hline
        \hline 
        3DConv & $0.39$ & $9.29 \times 10^{-2}$ & 78,150\\
        DeciWatch & $14.2$ & $0.11$  & 1,708,388 \\
        bi-Conv LSTM Model S & $10.6$ & $1.68 \times 10^{-2}$ & 1,875,666 \\
        bi-Conv LSTM Model C & $20.1$ & $6.93 \times 10^{-2}$ & 1,872313 \\
        \hline
    \end{tabular}
    \label{tab:model_measurements}
    \caption{Mean prediction time of each architecture on a single window from the whole finetune dataset, as well as the total number of parameters of each architecture. The time measurements are based on five runs and were measure on the machine described in section \ref{sec:finetune_tech_details}.}
\end{table}
\noindent As we have now fuly developed and tested all of our models on all three experiments, we can decide which models yield the best results for temporal smoothing in bouldering.
\\
\\
We have in Figure \ref{tab:finetune_mean_res} written the mean testing PCK$@0.05$, PCK$@0.1$, and PCK$@0.2$ of each finetuning run. Based on these numbers it seems like DeciWatch 1.1/1.2 would be the models to pick, as these yield the highest testing accuracy, based on the average PCK$@0.05$, PCK$@0.1$, and PCK$@0.2$ accuracies. However, as we know from section \ref{subsec:finetune_res_res}, DeciWatch 1.1/1.2 delivers some of the least optimal results when considering PCK$@0.2$. On the other hand, if we instead consider PCK$@0.05$, then DeciWatch is generally the model that delivers the greatest results and by quite a big margin.
\\
\\
Thus, if one is looking for the run that is on average delivering the greatest results, then DeciWatch 1.1 or 1.2 would be the ideal choice. However, if one is just looking for a rough estimation of the position of the keypoints, then bi-ConvLSTM Model C 1.1 would be the ideal choice, as we know from section \ref{subsec:finetune_test_res}, that this run delivers the highest PCK$@0.2$ testing accuracy, at the cost of one of the lowest PCK$@0.05$ testing accuracies.
\\
\\
One could also consider the prediciton time and the size of each model. We have in Table \ref{tab:model_measurements} written the mean prediction time of each model on a single window, as well as the size of each model. From the table we can clearly see, that 3DConv is by far the fastest and smallest architecture. This, combined with the general decent results of 3DConv in Table \ref{tab:finetune_mean_res}, could mean, that 3DConv could be the optimal choice if one is concerned about the prediction time or size of the model.


\subsection{General Reflections}
\label{sec:general_reflections}
Generally, we believe that we made the correct choices throughout the execution of the project. However, looking back throughout the project, we do find some bad decision made by us, that could have been avoided or made in another way.

\subsubsection{Pretraining}
For the preprocessing of the pretraining dataset, the goal was to preprocess the data, such that it simulated the output of the keypoint detector. This was done through multiple steps, where we used multiple values. For instance, when we expanded the bounding-boxes created from the keypoints by $10\%$ in each direction, we used a standard deviation in the range $\{1, 1.5, 2, 2.5, 3\}$ for the Gaussian filter on the input data, and we shifted the input by sampling from a normal distribution with mean $\mu_{in} = 1$ and standard deviation $3$ or $6$, depending on the experiment. These values were all some values that we came up with. We could instead have picked these values in a more sophisticated manner such that the data would be more similar to the ClimbAlong data. This could for instance have been done by approximating them from the ClimbAlong dataset. We could further have made this standard deviation keypoint-dependent, such that it would have modelled how much movement each keypoint makes. However, we do not too sure how beneficial this would have been, as (1) we generally already receive some great results which are difficult to beat, and (2) for the experiments where we increased the added noise, we did not see an improvement on of the keypoints that carry a lot of movement, for instance the pinky, hence why do not believe that more moving keypoints require being shifted more than less moving keypoints.
\\
\\
For the last part of the preprocessing of the pretraining dataset, we split the data into a training, validation and test set, consisting of $60\%$, $20\%$ and $20\%$ of the data respectively. This was done by making sure that none of the windows were repeated and that none of the windows were overlapping between the three datasets. By doing so we ensured that the evaluation carried minimal bias, as the models had not seen any of the frames in the validation and test set during its training. However, different frames of the same video sequence could appear across the three sets, which could potentially carry some bias, as the same person appear accross the multiple sets. Instead, it would most likely have been better if we made sure that the same sets had no overlapping video sequences, as this would lower the likelihood of introducing any evaluation bias.

\subsubsection{Finetuning}
When creating the target heatmaps for the finetuning data, we occasionally met a problem, where the groundtruth keypoints was placed outside of the predicted bounding-box, which was used as target bounding-box. As the goal of our models is not to correct the predicted bounding-box, but instead just to correct the position of the predicted keypoints, we had to make a choice of what to do for keypoints that were supposed to be placed outside of the bounding-box. We generally saw two solutions to the, as we could either (1) completely remove the keypoint and just leave the heatmap empty, or we could (2) move the keypoint such that it was somewhere inside the bounding-box. As we expected the first solution to have a negative impact, as the models would just learn that the keypoint was missing, we simply went with the second solution, however, one could argue that the first solution would be the optimal choice, as the models are essentially learning the incorrect position of the keypoints.


\subsection{Future Work}
\label{sec:future_work}
If we were to work further with this project, we first find it interesting to experiment with other machine learning methods. We would for instance find it very interesting to test the effects of letting DeciWatch process all frames, instead of only processing every fifth frame. Further, we find it interesting whether or not it would improve the results if DeciWatch was adapted, such that it made use of Vision Transformers as introduced by Dosovitskiy \textit{Et al}. \cite{dosovitskiy2021image}.
\\
\\
Further, we see some potential work in trying to avoid the overfitting during the finetuning-stage that we experienced in section \ref{sec:finetuning} and find it interesting how much this would improve the final results. We did attempt some of this in section \ref{sec:finetune_additional_experiment}, however, without any luck. One could for instance further try to avoid the overfitting by (1) increasing the variance of the dataset by incorporating even more data augmentation, for instance by adding some noise to the data, or (2) decrease the complexity of the models, such that they have less tuneable parameters
\\
\\
Lastly, we suggest that the models could be retrained multiple times. In section \ref{sec:results} we blaimed some of the results on the random initialization of the weights of the models. By retraining the models multiple times, the likelihood of this randomness being the reasoning for these results decreases and thus we will be sure whether or not our results are due to the randomness of the weights.

\end{document}